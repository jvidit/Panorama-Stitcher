{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from statistics import median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "nImages = None\n",
    "imageSetNumber = 1\n",
    "kFirstMatches = 2\n",
    "\n",
    "nKeyPointsInitial = 15000\n",
    "goodMatchRatio = 0.7\n",
    "\n",
    "histogramEqualizationWIndowSize = (2,2)\n",
    "xParameter=2.0\n",
    "\n",
    "BlurTuple = (5,5)\n",
    "\n",
    "resizeFactor = 5\n",
    "resizeTuple = (4160//resizeFactor, 2340//resizeFactor)\n",
    "\n",
    "\n",
    "k = 100\n",
    "ScaleCorrection = [[k,0,0],[0,k,0],[0,0,1]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageSetPath = \"./In-Sample/\"+str(imageSetNumber)+\"/\"\n",
    "\n",
    "\n",
    "images = [ cv2.imread((imageSetPath+image)) for image in os.listdir(imageSetPath) if image[-3:]==\"jpg\"]\n",
    "nImages = len(images)\n",
    "# for i in range(len(images)) :\n",
    "#     images[i] = cv2.cvtColor(images[i], cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# print(len(images))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(images)):\n",
    "    blur = cv2.blur(images[i], BlurTuple)\n",
    "    images[i] = cv2.resize(blur,resizeTuple)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Laplacian_Pyramid_Blending_with_mask(A, B, m, num_levels = 6):\n",
    "    # assume mask is float32 [0,1]\n",
    "\n",
    "    \n",
    "    k = 2**num_levels\n",
    "    A = A[:(A.shape[0]//k)*k, :(A.shape[1]//k)*k,:]\n",
    "    B = B[:(B.shape[0]//k)*k, :(B.shape[1]//k)*k,:]\n",
    "    m = m[:(m.shape[0]//k)*k, :(m.shape[1]//k)*k,:]\n",
    "\n",
    "    \n",
    "    # generate Gaussian pyramid for A,B and mask\n",
    "    GA = A.copy()\n",
    "    GB = B.copy()\n",
    "    GM = m.copy()\n",
    "    gpA = [GA]\n",
    "    gpB = [GB]\n",
    "    gpM = [GM]\n",
    "    for i in range(num_levels):\n",
    "        GA = cv2.pyrDown(GA)\n",
    "        GB = cv2.pyrDown(GB)\n",
    "        GM = cv2.pyrDown(GM)\n",
    "        gpA.append(np.float32(GA))\n",
    "        gpB.append(np.float32(GB))\n",
    "        gpM.append(np.float32(GM))\n",
    "\n",
    "    # generate Laplacian Pyramids for A,B and masks\n",
    "    lpA  = [gpA[num_levels-1]] # the bottom of the Lap-pyr holds the last (smallest) Gauss level\n",
    "    lpB  = [gpB[num_levels-1]]\n",
    "    gpMr = [gpM[num_levels-1]]\n",
    "    for i in range(num_levels-1,0,-1):\n",
    "        # Laplacian: subtarct upscaled version of lower level from current level\n",
    "        # to get the high frequencies\n",
    "        LA = np.subtract(gpA[i-1], cv2.pyrUp(gpA[i]))\n",
    "        LB = np.subtract(gpB[i-1], cv2.pyrUp(gpB[i]))\n",
    "        lpA.append(LA)\n",
    "        lpB.append(LB)\n",
    "        gpMr.append(gpM[i-1]) # also reverse the masks\n",
    "\n",
    "    # Now blend images according to mask in each level\n",
    "    LS = []\n",
    "    for la,lb,gm in zip(lpA,lpB,gpMr):\n",
    "        ls = (la * gm + lb * (255 - gm))/255\n",
    "        LS.append(ls)\n",
    "\n",
    "    # now reconstruct\n",
    "    ls_ = LS[0]\n",
    "    \n",
    "    for i in range(1,num_levels):\n",
    "        ls_ = cv2.pyrUp(ls_)\n",
    "        print(ls_.dtype, LS[i].dtype)\n",
    "        ls_ = (cv2.add(ls_, np.float32(LS[i])))\n",
    "\n",
    "    return ls_\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     A = cv2.imread(\"input1.png\",0)\n",
    "#     B = cv2.imread(\"input2.png\",0)\n",
    "#     m = np.zeros_like(A, dtype='float32')\n",
    "#     m[:,A.shape[1]/2:] = 1 # make the mask half-and-half\n",
    "#     lpb = Laplacian_Pyramid_Blending_with_mask(A, B, m, 5)\n",
    "#     cv2.imwrite(\"lpb.png\",lpb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatches(img1, img2):\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = False)\n",
    "    orb = cv2.ORB_create(nfeatures=nKeyPointsInitial)\n",
    "\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(img1, None)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(img2, None)\n",
    "    \n",
    "    matches = bf.knnMatch(descriptors1, descriptors2, k=kFirstMatches)\n",
    "    matches = selectGoodMatches(matches)\n",
    "    \n",
    "#     print(\"matches list is:\", matches)\n",
    "    \n",
    "    return matches, keypoints1, keypoints2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOrdering(images):\n",
    "    \n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = False)\n",
    "    orb = cv2.ORB_create(nfeatures=nKeyPointsInitial)\n",
    "    splitImages = []\n",
    "    for image in images:\n",
    "        cols = len(image[0])\n",
    "        splitImages.append(image[:,0:(cols//2),:])\n",
    "        splitImages.append(image[:,(cols//2):cols,:])\n",
    "    \n",
    "    images = splitImages\n",
    "    nImages=len(images)\n",
    "    \n",
    "    \n",
    "    maxIndices = None, None\n",
    "    maxMatches = []\n",
    "    \n",
    "\n",
    "    matchDictionary = {}\n",
    "    for i in range(nImages):\n",
    "        matchDictionary[i] = {}\n",
    "    \n",
    "    \n",
    "    keypoints = []\n",
    "    descriptors = []\n",
    "    \n",
    "    for image in images :\n",
    "        keypoints_orb, descriptors_orb = orb.detectAndCompute(image, None)\n",
    "        keypoints.append(keypoints_orb)\n",
    "        descriptors.append(descriptors_orb)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(nImages):\n",
    "        print(\"\\n\")\n",
    "        for j in range(i+1,nImages):\n",
    "            matches = bf.knnMatch(descriptors[i], descriptors[j], k=kFirstMatches)\n",
    "            matches = selectGoodMatches(matches)\n",
    "            matchDictionary[i][j] = matches\n",
    "            matchDictionary[j][i] = matches\n",
    "            print(i,j,len(matchDictionary[i][j]))\n",
    "        \n",
    "            \n",
    "    \n",
    "    \n",
    "    imageClusters = [[2*i, 2*i+1] for i in range(nImages//2)]\n",
    "    \n",
    "    for n in range(len(imageClusters),1,-1):\n",
    "        \n",
    "        closestClusters = None, None\n",
    "        maxMatch = 0\n",
    "        \n",
    "        print(imageClusters)\n",
    "        for i in range(len(imageClusters)):\n",
    "            img1 = imageClusters[i][-1]\n",
    "            for j in range(len(imageClusters)):\n",
    "                if(i!=j):\n",
    "                    img2 = imageClusters[j][0]\n",
    "                    if(len(matchDictionary[img1][img2])>maxMatch):\n",
    "                        maxMatch = len(matchDictionary[img1][img2])\n",
    "                        closestClusters = i, j\n",
    "#         print(closestClusters)\n",
    "        ind1, ind2 = closestClusters\n",
    "        imageClusters[ind1]+=imageClusters[ind2]\n",
    "        del imageClusters[ind2]\n",
    "    \n",
    "    returnIndices = [imageClusters[0][i]//2 for i in range(0,len(imageClusters[0]),2)]\n",
    "    print(returnIndices)\n",
    "    return returnIndices, matchDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectGoodMatches(matches):\n",
    "    # Apply ratio test\n",
    "    good = []\n",
    "    for m in matches:\n",
    "        if m[0].distance < goodMatchRatio*m[1].distance:\n",
    "            good.append(m[0])\n",
    "    matches = np.asarray(good)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgbEqualize(img):\n",
    "    img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "\n",
    "    # equalize the histogram of the Y channel\n",
    "#     img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=histogramEqualizationWIndowSize)\n",
    "    img_yuv[:,:,0] = clahe.apply(img_yuv[:,:,0])\n",
    "    # convert the YUV image back to RGB format\n",
    "    img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)\n",
    "    return img_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warpTwoImages(img1, img2):\n",
    "    '''warp img2 to img1 with homograph H'''\n",
    "    \n",
    "    matches, keypoints1, keypoints2 = getMatches(img1, img2)\n",
    "    \n",
    "    print(\"shape of matches is:\", matches.shape)\n",
    "    \n",
    "    src_pts = np.float32([ keypoints1[m.queryIdx].pt for m in matches ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([ keypoints2[m.trainIdx].pt for m in matches ]).reshape(-1,1,2)\n",
    "\n",
    "    print(len(src_pts),len(dst_pts))\n",
    "    H, mask1 = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC,xParameter)\n",
    "#     H = np.linalg.inv(ScaleCorrection)*H*ScaleCorrection\n",
    "    \n",
    "    \n",
    "    \n",
    "#     pts1 = np.float32([[0,0],[0,h1],[w1,h1],[w1,0]])\n",
    "#     pts2 = np.float32([[0,0],[0,h2],[w2,h2],[w2,0]])\n",
    "#     pts2_ = cv2.getAffineTransform(pts2, H)\n",
    "\n",
    "\n",
    "\n",
    "    h1,w1 = img1.shape[:2]\n",
    "    h2,w2 = img2.shape[:2]\n",
    "    pts1 = np.float32([[0,0],[0,h1],[w1,h1],[w1,0]]).reshape(-1,1,2)\n",
    "    pts2 = np.float32([[0,0],[0,h2],[w2,h2],[w2,0]]).reshape(-1,1,2)\n",
    "    pts2_ = cv2.perspectiveTransform(pts2, H)\n",
    "    transformedPoints = pts2_.reshape(4,2)\n",
    "#     print(pts2_.reshape(4,2))\n",
    "    pts = np.concatenate((pts1, pts2_), axis=0)\n",
    "    [xmin, ymin] = np.int32(pts.min(axis=0).ravel() - 0.5)\n",
    "    [xmax, ymax] = np.int32(pts.max(axis=0).ravel() + 0.5)\n",
    "    t = [-xmin,-ymin]\n",
    "    Ht = np.array([[1,0,t[0]],[0,1,t[1]],[0,0,1]]) # translate\n",
    "    \n",
    "    \n",
    "    translatedPoints = [[point[0]+t[0],point[1]+t[1]] for point in transformedPoints]\n",
    "#     for point in transformedPoints:\n",
    "#         x,y = point\n",
    "#         temparray = [x+t[0], y+t[1]]\n",
    "#         translatedPoints.append(temparray)\n",
    "\n",
    "    \n",
    "    warpedImage = cv2.warpPerspective(img2, Ht.dot(H), (xmax-xmin, ymax-ymin))\n",
    "    \n",
    "    \n",
    "    mask = np.zeros(warpedImage.shape, dtype=np.uint8)\n",
    "    cv2.fillPoly(mask, np.array([translatedPoints], dtype=np.int32), (255, 255, 255))\n",
    "    \n",
    "    result = np.zeros(warpedImage.shape, dtype=np.uint8)\n",
    "    result[t[1]:h1+t[1],t[0]:w1+t[0]] = img1\n",
    "#     result[np.where(mask == 255)] = warpedImage[np.where(mask == 255)]\n",
    "    \n",
    "    A = warpedImage\n",
    "    B = result\n",
    "    m = mask\n",
    "    lpb = Laplacian_Pyramid_Blending_with_mask(A, B, m, 7)\n",
    "    result = lpb\n",
    "    \n",
    "    result = rgbEqualize(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeImages(images):\n",
    "    for i in range(nImages):\n",
    "        cv2.imwrite(\"log\"+str(i)+\".jpg\",images[i])\n",
    "        \n",
    "        \n",
    "writeImages(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0 1 12\n",
      "0 2 2\n",
      "0 3 9\n",
      "0 4 8\n",
      "0 5 11\n",
      "0 6 345\n",
      "0 7 1277\n",
      "\n",
      "\n",
      "1 2 1656\n",
      "1 3 10\n",
      "1 4 522\n",
      "1 5 9\n",
      "1 6 11\n",
      "1 7 333\n",
      "\n",
      "\n",
      "2 3 4\n",
      "2 4 426\n",
      "2 5 9\n",
      "2 6 8\n",
      "2 7 370\n",
      "\n",
      "\n",
      "3 4 1887\n",
      "3 5 231\n",
      "3 6 5\n",
      "3 7 7\n",
      "\n",
      "\n",
      "4 5 9\n",
      "4 6 3\n",
      "4 7 10\n",
      "\n",
      "\n",
      "5 6 3\n",
      "5 7 7\n",
      "\n",
      "\n",
      "6 7 1\n",
      "\n",
      "\n",
      "[[0, 1], [2, 3], [4, 5], [6, 7]]\n",
      "[[0, 1], [2, 3, 4, 5], [6, 7]]\n",
      "[[0, 1, 2, 3, 4, 5], [6, 7]]\n",
      "[3, 0, 1, 2]\n",
      "2\n",
      "shape of matches is: (1367,)\n",
      "1367 1367\n",
      "float32 float32\n",
      "float32 float32\n",
      "float32 float32\n",
      "float32 float32\n",
      "float32 float32\n",
      "float32 float32\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.1) /tmp/opencv-20190908-76835-1lnncvq/opencv-4.1.1/modules/imgproc/src/clahe.cpp:351: error: (-215:Assertion failed) _src.type() == CV_8UC1 || _src.type() == CV_16UC1 in function 'apply'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-386-bc85f06192d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhorizontalStitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-386-bc85f06192d7>\u001b[0m in \u001b[0;36mhorizontalStitch\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mrightStitchedImage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstitchFromRight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morderedImages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcenterImageIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morderedImages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcenterImageIndex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mleftStitchedImage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstitchFromLeft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrightStitchedImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morderedImages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcenterImageIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mleftStitchedImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-386-bc85f06192d7>\u001b[0m in \u001b[0;36mstitchFromLeft\u001b[0;34m(image, images)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mwarpedImg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwarpTwoImages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstitchFromLeft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarpedImg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-384-05a66f00dd15>\u001b[0m in \u001b[0;36mwarpTwoImages\u001b[0;34m(img1, img2)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlpb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgbEqualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-383-181f4a5a5d61>\u001b[0m in \u001b[0;36mrgbEqualize\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mclahe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateCLAHE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipLimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtileGridSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistogramEqualizationWIndowSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mimg_yuv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclahe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_yuv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# convert the YUV image back to RGB format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mimg_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_yuv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_YUV2BGR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.1.1) /tmp/opencv-20190908-76835-1lnncvq/opencv-4.1.1/modules/imgproc/src/clahe.cpp:351: error: (-215:Assertion failed) _src.type() == CV_8UC1 || _src.type() == CV_16UC1 in function 'apply'\n"
     ]
    }
   ],
   "source": [
    "def horizontalStitch(images):\n",
    "    stitchOrder, matches = getOrdering(images)\n",
    "    orderedImages = []\n",
    "    for i in stitchOrder:\n",
    "        orderedImages.append(images[i])\n",
    "    centerImageIndex = len(orderedImages)//2\n",
    "    print(centerImageIndex)\n",
    "    \n",
    "#     img = orderedImages[centerImageIndex]\n",
    "#     for i in range(centerImageIndex+1,centerImageIndex+2):#len(orderedImages)):\n",
    "#         img = warpTwoImages(img, orderedImages[i])\n",
    "        \n",
    "#     for i in range(centerImageIndex-1,centerImageIndex-2,-1):#len(orderedImages)):\n",
    "#         img = warpTwoImages(img, orderedImages[i])\n",
    "    \n",
    "    \n",
    "    rightStitchedImage = stitchFromRight(orderedImages[centerImageIndex], orderedImages[centerImageIndex+1:])\n",
    "    leftStitchedImage = stitchFromLeft(rightStitchedImage, orderedImages[:centerImageIndex]) \n",
    "    \n",
    "    return leftStitchedImage\n",
    "#     return img\n",
    "\n",
    "    \n",
    "def stitchFromRight(image, images):\n",
    "    if (len(images) == 1):\n",
    "        return image\n",
    "    warpedImg = warpTwoImages(image, images[0])\n",
    "    return stitchFromRight(warpedImg, images[1:])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def stitchFromLeft(image, images):\n",
    "    if (len(images) == 1):\n",
    "        return image\n",
    "    warpedImg = warpTwoImages(image, images[-1])\n",
    "    return stitchFromLeft(warpedImg, images[:-1])\n",
    "    \n",
    "    \n",
    "result = horizontalStitch(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"log.jpg\",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
